{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "049ffcc1",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c84e4de",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ccda3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/sentiment/train.csv\")\n",
    "df.drop_duplicates(\"text\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "10fce65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "48cc960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5994c912",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do not have any standardized meaning under ifrs adjusted gross margin, adjusted gross margin % and adjusted ebitda are non - ifrs financial measures not defined by and don't have any standardized meaning under ifrs.\n",
      "do not regain significant control the bulls don't regain significant control without reclaiming its 200 - day moving average ( which would put it above the rest of its daily moving averages ) until that happens, it remains a two - way market.\n",
      "do not i don't use \" fibs \" in the \" traditional \" manner ( retracements ).\n",
      "' s \" impressive mental gymnastics \" jackson palmer, co - creator of dogecoin, says it's \" impressive mental gymnastics \" to associate \" freedom \" with elon musk's bid to buy twitter.\n",
      "' extraordinarily elevated'inflation numbers the white house is bracing for'extraordinarily elevated'inflation numbers to be reflected in tuesday's data from the labor department, which will be released on tuesday.\n",
      "do not have enough throughput he suggested that current blockchain networks don't have enough throughput to handle large - scale gaming experiences.\n",
      "do not want your eth, i want your input on how to design om the om metaverse platform's creator noted that i don't want your eth, i want your input on how to design om.\n",
      "' disrupting the economic system ' estavi was just released from prison in iran, where he spent nine months on charges of'disrupting the economic system '\n",
      "' s deepening its partnership microsoft recently announced that it's deepening its partnership with boeing.\n",
      "' s price - to - book - book ratio is below 3. 2. 3 the company's price - to - book - book ratio is below 3. 2. 3.\n",
      "is a look at the'january's'gigantic gme short squeeze ' the sneeze report is a look at the'january's gigantic gme short squeeze and \" operation infiltration'of social media.\n",
      "' re based in the world of the archie horror imprint and inspired by \" the chilling adventures of sabrina, \" which became a netflix series. they're based in the world of the archie horror imprint and inspired by \" the chilling adventures of sabrina, \" which became a netflix series.\n",
      "do not believe these decisions will be primary drivers of the appreciation in tesla's stock price. we don't believe these decisions will be primary drivers of the appreciation in tesla's stock price.\n",
      "' s reined in the power it's reined in the power of companies from alibaba to ride - sharing provider didi global inc.\n",
      "' o buds ', pink cookies, big bag'o'buds product line'o buds ', pink cookies, and big bag o'line extension.\n"
     ]
    }
   ],
   "source": [
    "tdata = []\n",
    "\n",
    "for row in train.itertuples():\n",
    "    text = tokenizer.decode(tokenizer(row.text)[\"input_ids\"][1:-1])\n",
    "    encoded = tokenizer(text)\n",
    "    tokens = encoded[\"input_ids\"]\n",
    "\n",
    "    l = 0\n",
    "    res = \"\"\n",
    "    part = tokenizer.decode(tokenizer(row.important_span_text)[\"input_ids\"][1:-1])\n",
    "    if part not in text:\n",
    "        print(part, text)\n",
    "        continue\n",
    "    \n",
    "    start = text.index(part)\n",
    "    end = start + len(part)\n",
    "    decoded = []\n",
    "    \n",
    "    y = [0]\n",
    "    p = 1 if row.sentiment == \"positive\" else 2\n",
    "    \n",
    "    for i, tok in enumerate(tokens[1:-1]):\n",
    "        dec = tokenizer.decode(tok, clean_up_tokenization_spaces=False)\n",
    "        if dec.startswith(\"##\"):\n",
    "            dec = dec.replace(\"##\", \"\")\n",
    "        elif i != 0:\n",
    "            dec = \" \" + dec\n",
    "        \n",
    "        decoded.append(dec)\n",
    "\n",
    "        if start - 1 <= l < end:\n",
    "            res = res + dec\n",
    "            y.append(p)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        \n",
    "        l += len(dec)\n",
    "\n",
    "    res = res[1:] if res.startswith(\" \") else res\n",
    "    y.append(0)\n",
    "    #assert l == len(text), decoded\n",
    "    #assert res == part\n",
    "    \n",
    "    encoded[\"labels\"] = y\n",
    "    tdata.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "40f4c547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'has gifted the luna foundation guard ( lfg ) 10 million luna'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3aea014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjusted gross margin, adjusted gross margin % and adjusted ebitda are non - ifrs financial measures not defined by and don't have any standardized meaning under ifrs.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a069dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 167\n"
     ]
    }
   ],
   "source": [
    "print(l, len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1fe65df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "41bc6824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas(Index=3445, text='Adjusted gross margin, adjusted gross margin % and adjusted EBITDA are non-IFRS financial measures not defined by and do not have any standardized meaning under IFRS.', start_char_pos=118, end_char_pos=163, important_span_text='do not have any standardized meaning under IFRS', sentiment='negative')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "494ae95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vulnerable ' annual report :'vulnerable'and the impact of the political and security situation in israel on our business.\n",
      "fda approved biocryst pharmaceuticals ' in december 2020, the fda approved biocryst pharmaceuticals'orladeyo ( berotralstat ) for hae in adults and patients 12 years and older.\n"
     ]
    }
   ],
   "source": [
    "vdata = []\n",
    "\n",
    "for row in valid.itertuples():\n",
    "    text = tokenizer.decode(tokenizer(row.text)[\"input_ids\"][1:-1])\n",
    "    encoded = tokenizer(text)\n",
    "    tokens = encoded[\"input_ids\"]\n",
    "\n",
    "    l = 0\n",
    "    res = \"\"\n",
    "    part = tokenizer.decode(tokenizer(row.important_span_text)[\"input_ids\"][1:-1])\n",
    "    if part not in text:\n",
    "        print(part, text)\n",
    "        continue\n",
    "    \n",
    "    start = text.index(part)\n",
    "    end = start + len(part)\n",
    "    decoded = []\n",
    "    \n",
    "    y = [0]\n",
    "    p = 1 if row.sentiment == \"positive\" else 2\n",
    "    \n",
    "    for i, tok in enumerate(tokens[1:-1]):\n",
    "        dec = tokenizer.decode(tok, clean_up_tokenization_spaces=False)\n",
    "        if dec.startswith(\"##\"):\n",
    "            dec = dec.replace(\"##\", \"\")\n",
    "        elif i != 0:\n",
    "            dec = \" \" + dec\n",
    "        \n",
    "        decoded.append(dec)\n",
    "\n",
    "        if start - 1 <= l < end:\n",
    "            res = res + dec\n",
    "            y.append(p)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        \n",
    "        l += len(dec)\n",
    "\n",
    "    res = res[1:] if res.startswith(\" \") else res\n",
    "    y.append(0)\n",
    "    #assert l == len(text), decoded\n",
    "    #assert res == part\n",
    "    \n",
    "    encoded[\"labels\"] = y\n",
    "    vdata.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0e0016f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7645 850\n"
     ]
    }
   ],
   "source": [
    "print(len(tdata), len(vdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "724690ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import create_optimizer\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c7bbecad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForTokenClassification.from_pretrained(\"ProsusAI/finbert\", from_pt=True, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3484113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4a0c5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9ac97bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(pd.DataFrame(tdata))\n",
    "ds_val = Dataset.from_pandas(pd.DataFrame(vdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "eda214b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5ba6095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = ds_train.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_valid = ds_val.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d66933d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_train_epochs = 1\n",
    "num_train_steps = (len(tdata) // batch_size) * num_train_epochs\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d00ad8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "02320bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 1413s 6s/step - loss: 0.5298 - val_loss: 0.4134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcf07137c40>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=tf_train, validation_data=tf_valid, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4cf77d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "03e0b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5bea1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sentiment/testing.json\") as infile:\n",
    "    data = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "67113445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, \"x\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7958e92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [25:22<00:00,  6.57it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for row in tqdm.tqdm(data, total=len(data)):\n",
    "    all_options = []\n",
    "    res = \"\"\n",
    "    text = row[\"text\"]\n",
    "    text = text.replace(\"’\", \"'\").replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"–\",\"-\").replace(\"…\", \".\").replace(\"‘\", \"'\").replace(\"—\", \"-\")\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    for word in nlp(text):\n",
    "        if word[\"entity\"] != \"LABEL_0\":\n",
    "            res = res + word[\"word\"].replace(\"Ġ\", \" \")\n",
    "        else:\n",
    "            all_options.append(res)\n",
    "            res = \"\"\n",
    "\n",
    "    all_options.append(res)\n",
    "    sel = max(all_options, key=lambda x: len(x))\n",
    "    cleaned = re.sub(\"(^[\\:, ]+)|([\\. ]+$)\", \"\", sel)\n",
    "    start = 0\n",
    "    end = 0\n",
    "    if len(cleaned) > 1:\n",
    "        if cleaned in text:\n",
    "            start = text.index(cleaned)\n",
    "            end = start + len(cleaned)\n",
    "        #else:\n",
    "        #    end = cleaned\n",
    "    results.append((start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "33439fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data=results, columns=[\"start\", \"end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "39cd4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.reset_index().rename(columns={\"index\": \"id\"}).to_csv(\"../predictions/t2_roberta_tokenizer.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
