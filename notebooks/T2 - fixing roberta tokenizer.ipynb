{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "049ffcc1",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c84e4de",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccda3450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/sentiment/train.csv\")\n",
    "df.drop_duplicates(\"text\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10fce65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48cc960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5994c912",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 23:58:53.847890: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-29 23:58:53.847910: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "tdata = []\n",
    "\n",
    "for row in train.itertuples():\n",
    "    encoded = tokenizer(row.text)\n",
    "    tokens = encoded[\"input_ids\"]\n",
    "\n",
    "    l = 0\n",
    "    res = \"\"\n",
    "    start = row.text.index(row.important_span_text)\n",
    "    end = start + len(row.important_span_text)\n",
    "    decoded = []\n",
    "    \n",
    "    y = [0]\n",
    "    p = 1 if row.sentiment == \"positive\" else 2\n",
    "    \n",
    "    assert row.important_span_text in row.text\n",
    "    \n",
    "    for tok in tokens[1:-1]:\n",
    "        dec = tokenizer.decode(tok)\n",
    "        decoded.append(dec)\n",
    "\n",
    "        if start - 1 <= l < end:\n",
    "            res = res + dec\n",
    "            y.append(p)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        \n",
    "        l += len(dec)\n",
    "\n",
    "    res = res[1:] if res.startswith(\" \") else res\n",
    "    y.append(0)\n",
    "    assert l == len(row.text), decoded\n",
    "    assert res == row.important_span_text\n",
    "    \n",
    "    encoded[\"labels\"] = y\n",
    "    tdata.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494ae95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdata = []\n",
    "\n",
    "for row in valid.itertuples():\n",
    "    encoded = tokenizer(row.text)\n",
    "    tokens = encoded[\"input_ids\"]\n",
    "\n",
    "    l = 0\n",
    "    res = \"\"\n",
    "    start = row.text.index(row.important_span_text)\n",
    "    end = start + len(row.important_span_text)\n",
    "    decoded = []\n",
    "    \n",
    "    y = [0]\n",
    "    p = 1 if row.sentiment == \"positive\" else 2\n",
    "    \n",
    "    assert row.important_span_text in row.text\n",
    "    \n",
    "    for tok in tokens[1:-1]:\n",
    "        dec = tokenizer.decode(tok)\n",
    "        decoded.append(dec)\n",
    "\n",
    "        if start - 1 <= l < end:\n",
    "            res = res + dec\n",
    "            y.append(p)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        \n",
    "        l += len(dec)\n",
    "\n",
    "    res = res[1:] if res.startswith(\" \") else res\n",
    "    y.append(0)\n",
    "    assert l == len(row.text), decoded\n",
    "    assert res == row.important_span_text\n",
    "    \n",
    "    encoded[\"labels\"] = y\n",
    "    vdata.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e0016f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7660 852\n"
     ]
    }
   ],
   "source": [
    "print(len(tdata), len(vdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724690ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import create_optimizer\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7bbecad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 23:58:57.368469: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-29 23:58:57.368488: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-29 23:58:57.368502: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (eka-thinkpad): /proc/driver/nvidia/version does not exist\n",
      "2022-04-29 23:58:57.368656: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFRobertaForTokenClassification.\n",
      "\n",
      "Some layers of TFRobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForTokenClassification.from_pretrained(\"roberta-base\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3484113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a0c5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac97bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(pd.DataFrame(tdata))\n",
    "ds_val = Dataset.from_pandas(pd.DataFrame(vdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eda214b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba6095c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evaldas/miniconda3/envs/rnd/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "tf_train = ds_train.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_valid = ds_val.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d66933d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_train_epochs = 5\n",
    "num_train_steps = (len(tdata) // batch_size) * num_train_epochs\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d00ad8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02320bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 23:58:59.064111: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "239/239 [==============================] - 1292s 5s/step - loss: 0.5064 - val_loss: 0.3884\n",
      "Epoch 2/5\n",
      "239/239 [==============================] - 1210s 5s/step - loss: 0.3424 - val_loss: 0.3378\n",
      "Epoch 3/5\n",
      "239/239 [==============================] - 1149s 5s/step - loss: 0.2781 - val_loss: 0.3329\n",
      "Epoch 4/5\n",
      "239/239 [==============================] - 1128s 5s/step - loss: 0.2301 - val_loss: 0.3450\n",
      "Epoch 5/5\n",
      "239/239 [==============================] - 1125s 5s/step - loss: 0.1985 - val_loss: 0.3430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2030303a90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=tf_train, validation_data=tf_valid, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cf77d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03e0b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bea1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sentiment/testing.json\") as infile:\n",
    "    data = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67113445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, \"x\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7958e92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [22:21<00:00,  7.46it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for row in tqdm.tqdm(data, total=len(data)):\n",
    "    all_options = []\n",
    "    res = \"\"\n",
    "    text = row[\"text\"]\n",
    "    text = text.replace(\"’\", \"'\").replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"–\",\"-\").replace(\"…\", \".\").replace(\"‘\", \"'\").replace(\"—\", \"-\")\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    for word in nlp(text):\n",
    "        if word[\"entity\"] != \"LABEL_0\":\n",
    "            res = res + word[\"word\"].replace(\"Ġ\", \" \")\n",
    "        else:\n",
    "            all_options.append(res)\n",
    "            res = \"\"\n",
    "\n",
    "    all_options.append(res)\n",
    "    sel = max(all_options, key=lambda x: len(x))\n",
    "    cleaned = re.sub(\"(^[\\:, ]+)|([\\. ]+$)\", \"\", sel)\n",
    "    start = 0\n",
    "    end = 0\n",
    "    if len(cleaned) > 1:\n",
    "        if cleaned in text:\n",
    "            start = text.index(cleaned)\n",
    "            end = start + len(cleaned)\n",
    "        #else:\n",
    "        #    end = cleaned\n",
    "    results.append((start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33439fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data=results, columns=[\"start\", \"end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "39cd4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.reset_index().rename(columns={\"index\": \"id\"}).to_csv(\"../predictions/t2_roberta_tokenizer.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
